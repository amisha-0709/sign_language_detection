{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5bdb97b-6e72-46b9-b271-77770e0c017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6857a48c-a29b-4335-b4bb-e6f2aea55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 398, 398, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 199, 199, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 197, 197, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 98, 98, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 96, 96, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 48, 48, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 23, 23, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 8464)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               1083520   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 96)                12384     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 96)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,119,720\n",
      "Trainable params: 1,119,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('A:\\desktop\\progit\\Sign-Language-To-Text-and-Speech-Conversion\\cnn8grps_rad1_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38099b92-b4c1-40a7-b8c7-a8076c0f4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'A:\\desktop\\progit\\Sign-Language-To-Text-and-Speech-Conversion\\AtoZ_3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b9cfcb-c7a9-4abb-a97e-6b7b722359c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Use 20% of data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9826f8d-8665-4f3b-87b5-7f7edecb123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4681 images belonging to 26 classes.\n",
      "Found 4681 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "# # Flow training images in batches of 32 using train_datagen generator\n",
    "# train_generator = datagen.flow_from_directory(\n",
    "#     data_dir,\n",
    "#     target_size=(224, 224),  # Adjust to your model's input size\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     subset='training'  # Set as training data\n",
    "# )\n",
    "\n",
    "# # Flow validation images in batches of 32 using validation_datagen generator\n",
    "# validation_generator = datagen.flow_from_directory(\n",
    "#     data_dir,\n",
    "#     target_size=(224, 224),  # Adjust to your model's input size\n",
    "#     batch_size=32,\n",
    "#     class_mode='categorical',\n",
    "#     subset='validation'  # Set as validation data\n",
    "# )\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(398, 398),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(398, 398),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce1c1e5a-54d4-4f75-bfef-6c7c8b3be8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the input shape\n",
    "input_shape = (398, 398, 3)  # Update this if necessary\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2451579f-ad04-4b4b-a5fd-0ce71ed206ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 396, 396, 32)\n",
      "(None, 198, 198, 32)\n",
      "(None, 196, 196, 32)\n",
      "(None, 98, 98, 32)\n",
      "(None, 96, 96, 16)\n",
      "(None, 48, 48, 16)\n",
      "(None, 46, 46, 16)\n",
      "(None, 23, 23, 16)\n",
      "(None, 8464)\n",
      "(None, 128)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e98c8f-c439-4b80-a8b2-82595290da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "146/146 [==============================] - 53s 340ms/step - loss: -30008877056.0000 - accuracy: 0.0383 - val_loss: -280942346240.0000 - val_accuracy: 0.0383\n",
      "Epoch 2/10\n",
      "146/146 [==============================] - 28s 194ms/step - loss: -14403222110208.0000 - accuracy: 0.0387 - val_loss: -62673848893440.0000 - val_accuracy: 0.0385\n",
      "Epoch 3/10\n",
      "146/146 [==============================] - 27s 187ms/step - loss: -410475527405568.0000 - accuracy: 0.0385 - val_loss: -1143260567306240.0000 - val_accuracy: 0.0385\n",
      "Epoch 4/10\n",
      "146/146 [==============================] - 30s 204ms/step - loss: -3646937798541312.0000 - accuracy: 0.0385 - val_loss: -7887022391820288.0000 - val_accuracy: 0.0385\n",
      "Epoch 5/10\n",
      "146/146 [==============================] - 31s 210ms/step - loss: -17957580387123200.0000 - accuracy: 0.0383 - val_loss: -32943861596684288.0000 - val_accuracy: 0.0385\n",
      "Epoch 6/10\n",
      "146/146 [==============================] - 30s 206ms/step - loss: -61804497785061376.0000 - accuracy: 0.0387 - val_loss: -101711757646495744.0000 - val_accuracy: 0.0383\n",
      "Epoch 7/10\n",
      "146/146 [==============================] - 31s 210ms/step - loss: -168744128278953984.0000 - accuracy: 0.0385 - val_loss: -256518072804835328.0000 - val_accuracy: 0.0383\n",
      "Epoch 8/10\n",
      "146/146 [==============================] - 46s 315ms/step - loss: -390397237122826240.0000 - accuracy: 0.0385 - val_loss: -558791016329510912.0000 - val_accuracy: 0.0381\n",
      "Epoch 9/10\n",
      "146/146 [==============================] - 139s 957ms/step - loss: -802536079734341632.0000 - accuracy: 0.0383 - val_loss: -1094455317397241856.0000 - val_accuracy: 0.0385\n",
      "Epoch 10/10\n",
      "146/146 [==============================] - 98s 669ms/step - loss: -1495924189935173632.0000 - accuracy: 0.0383 - val_loss: -1974442645459566592.0000 - val_accuracy: 0.0385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25a6b77ec50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa9e72bd-9735-4e81-b516-31ca8db68545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('fine_tune_models', exist_ok=True)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save('fine_tune_models/fintune1.h5')\n",
    "\n",
    "# Load the model from the HDF5 file to verify\n",
    "loaded_model = load_model('fine_tune_models/fintune1.h5')\n",
    "\n",
    "# Use the loaded model (example)\n",
    "# predictions = loaded_model.predict(some_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a641829-c8f9-4452-9807-f3641e356d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 13s 85ms/step - loss: -1973768095075926016.0000 - accuracy: 0.0385\n",
      "Validation Loss: -1.973768095075926e+18\n",
      "Validation Accuracy: 0.038453321903944016\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "validation_loss, validation_accuracy = loaded_model.evaluate(validation_generator)\n",
    "\n",
    "print(f\"Validation Loss: {validation_loss}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa988fba-4bf0-42e8-8cab-8a279c2a38a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 396, 396, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 198, 198, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 196, 196, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 98, 98, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 96, 96, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 48, 48, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 23, 23, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 8464)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               1083520   \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,100,737\n",
      "Trainable params: 1,100,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 4681 images belonging to 26 classes.\n",
      "Found 4681 images belonging to 26 classes.\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 28s 185ms/step - loss: -30926025127106707456.0000 - accuracy: 0.0385 - val_loss: -33339028738586705920.0000 - val_accuracy: 0.0385\n",
      "Epoch 2/10\n",
      " 32/147 [=====>........................] - ETA: 11s - loss: -33082651214291664896.0000 - accuracy: 0.0420"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 377, in _get_batches_of_transformed_samples\n    x = image_utils.img_to_array(img, data_format=self.data_format)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 324, in img_to_array\n    x = np.asarray(img, dtype=dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 377, in _get_batches_of_transformed_samples\n    x = image_utils.img_to_array(img, data_format=self.data_format)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 324, in img_to_array\n    x = np.asarray(img, dtype=dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_36611]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Save the entire model to a HDF5 file\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_tune_models/fintune1.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 377, in _get_batches_of_transformed_samples\n    x = image_utils.img_to_array(img, data_format=self.data_format)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 324, in img_to_array\n    x = np.asarray(img, dtype=dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[IteratorGetNext/_2]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) RESOURCE_EXHAUSTED:  MemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\amish\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 903, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 1050, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\preprocessing\\image.py\", line 377, in _get_batches_of_transformed_samples\n    x = image_utils.img_to_array(img, data_format=self.data_format)\n\n  File \"C:\\Users\\amish\\.conda\\envs\\py3102\\lib\\site-packages\\keras\\utils\\image_utils.py\", line 324, in img_to_array\n    x = np.asarray(img, dtype=dtype)\n\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.81 MiB for an array with shape (398, 398, 3) and data type float32\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_36611]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs('fine_tune_models', exist_ok=True)\n",
    "\n",
    "# Assuming the model is already defined and compiled\n",
    "# model = ...\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Adjust data generators if necessary\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(data_dir, target_size=(398, 398), batch_size=32, class_mode='binary')\n",
    "validation_generator = validation_datagen.flow_from_directory(data_dir, target_size=(398, 398), batch_size=32, class_mode='binary')\n",
    "\n",
    "# Compile the model with an appropriate loss function and metrics\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save('fine_tune_models/fintune1.h5')\n",
    "\n",
    "# Load the model from the HDF5 file to verify\n",
    "loaded_model = load_model('fine_tune_models/fintune1.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "validation_loss, validation_accuracy = loaded_model.evaluate(validation_generator)\n",
    "\n",
    "print(f\"Validation Loss: {validation_loss}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2caf04c-d875-40d6-b670-93818c24e9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4681 images belonging to 26 classes.\n",
      "Found 4681 images belonging to 26 classes.\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 11s 36ms/step - loss: -8015637504.0000 - accuracy: 0.0385 - val_loss: -58586001408.0000 - val_accuracy: 0.0385\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: -901236850688.0000 - accuracy: 0.0385 - val_loss: -2914092318720.0000 - val_accuracy: 0.0385\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: -10477169016832.0000 - accuracy: 0.0385 - val_loss: -23204594712576.0000 - val_accuracy: 0.0385\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 10s 36ms/step - loss: -52259106848768.0000 - accuracy: 0.0385 - val_loss: -93711044182016.0000 - val_accuracy: 0.0385\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 11s 38ms/step - loss: -168119767138304.0000 - accuracy: 0.0385 - val_loss: -264904204353536.0000 - val_accuracy: 0.0385\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - 10s 35ms/step - loss: -417542963200000.0000 - accuracy: 0.0385 - val_loss: -604945002790912.0000 - val_accuracy: 0.0385\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: -873694461165568.0000 - accuracy: 0.0385 - val_loss: -1191794100404224.0000 - val_accuracy: 0.0385\n",
      "Epoch 8/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: -1620243529072640.0000 - accuracy: 0.0385 - val_loss: -2114060615155712.0000 - val_accuracy: 0.0385\n",
      "Epoch 9/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: -2752768020316160.0000 - accuracy: 0.0385 - val_loss: -3474349906460672.0000 - val_accuracy: 0.0385\n",
      "Epoch 10/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: -4369410353528832.0000 - accuracy: 0.0385 - val_loss: -5365725191471104.0000 - val_accuracy: 0.0385\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Reduce image size\n",
    "input_size = (128, 128)\n",
    "\n",
    "# Define a simpler model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators with smaller batch size and target size\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save('fine_tune_models/fintune1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "600e766d-3fec-4c75-8a40-8e90c8c376e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4681 images belonging to 26 classes.\n",
      "Found 4681 images belonging to 26 classes.\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: 1.0836 - accuracy: 0.6928 - val_loss: 0.1485 - val_accuracy: 0.9615\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0954 - accuracy: 0.9776 - val_loss: 0.0281 - val_accuracy: 0.9932\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: 0.0351 - accuracy: 0.9895 - val_loss: 0.0101 - val_accuracy: 0.9985\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0323 - accuracy: 0.9936 - val_loss: 0.0313 - val_accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 10s 34ms/step - loss: 0.0274 - accuracy: 0.9925 - val_loss: 0.0086 - val_accuracy: 0.9987\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 0.0143 - accuracy: 0.9968 - val_loss: 0.0169 - val_accuracy: 0.9953\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - 10s 35ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 3.8449e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 1.7191e-04 - accuracy: 1.0000 - val_loss: 9.1292e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 7.6439e-05 - accuracy: 1.0000 - val_loss: 5.7299e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "293/293 [==============================] - 10s 33ms/step - loss: 5.1774e-05 - accuracy: 1.0000 - val_loss: 4.1754e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Reduce image size\n",
    "input_size = (128, 128)\n",
    "\n",
    "# Define the corrected model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(26, activation='softmax')  # 26 units for 26 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators with smaller batch size and target size\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'  # Categorical for multi-class classification\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'  # Categorical for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save('fine_tune_models/fintune2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62a6e2e4-5b13-40cb-90da-e4d1661cd29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4681 images belonging to 26 classes.\n",
      "Found 4681 images belonging to 26 classes.\n",
      "Epoch 1/10\n",
      "293/293 [==============================] - 22s 72ms/step - loss: 3.2602 - accuracy: 0.0440 - val_loss: 3.2203 - val_accuracy: 0.0743\n",
      "Epoch 2/10\n",
      "293/293 [==============================] - 22s 74ms/step - loss: 3.1277 - accuracy: 0.0733 - val_loss: 2.7467 - val_accuracy: 0.1380\n",
      "Epoch 3/10\n",
      "293/293 [==============================] - 21s 70ms/step - loss: 2.9211 - accuracy: 0.1111 - val_loss: 2.4422 - val_accuracy: 0.2405\n",
      "Epoch 4/10\n",
      "293/293 [==============================] - 20s 70ms/step - loss: 2.7263 - accuracy: 0.1521 - val_loss: 2.1457 - val_accuracy: 0.3036\n",
      "Epoch 5/10\n",
      "293/293 [==============================] - 21s 73ms/step - loss: 2.5610 - accuracy: 0.1869 - val_loss: 1.9215 - val_accuracy: 0.3461\n",
      "Epoch 6/10\n",
      "293/293 [==============================] - 21s 71ms/step - loss: 2.4008 - accuracy: 0.2156 - val_loss: 1.6940 - val_accuracy: 0.4070\n",
      "Epoch 7/10\n",
      "293/293 [==============================] - 21s 71ms/step - loss: 2.2833 - accuracy: 0.2405 - val_loss: 1.5013 - val_accuracy: 0.4839\n",
      "Epoch 8/10\n",
      "293/293 [==============================] - 21s 71ms/step - loss: 2.2013 - accuracy: 0.2683 - val_loss: 1.5281 - val_accuracy: 0.4830\n",
      "Epoch 9/10\n",
      "293/293 [==============================] - 21s 72ms/step - loss: 2.1016 - accuracy: 0.2784 - val_loss: 1.2571 - val_accuracy: 0.6242\n",
      "Epoch 10/10\n",
      "293/293 [==============================] - 21s 70ms/step - loss: 2.0786 - accuracy: 0.2871 - val_loss: 1.2749 - val_accuracy: 0.5975\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Reduce image size\n",
    "input_size = (128, 128)\n",
    "\n",
    "# Define the model with dropout\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(input_size[0], input_size[1], 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Add dropout\n",
    "    Dense(26, activation='softmax')  # 26 units for 26 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data generators with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'  # Categorical for multi-class classification\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=input_size,\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'  # Categorical for multi-class classification\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Save the entire model to a HDF5 file\n",
    "model.save('fine_tune_models/fintune3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a83b51-a203-4535-ade6-495ae59dbc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
